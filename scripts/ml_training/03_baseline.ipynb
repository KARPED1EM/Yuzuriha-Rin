{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b849b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å¼€å§‹ hfl/chinese-bert-wwm-ext åŸºçº¿è®­ç»ƒï¼ˆNotebook ç‰ˆï¼‰\n",
      "\n",
      "\n",
      "ğŸ“Š å¼€å§‹å‡†å¤‡æ•°æ®é›†...\n",
      "ğŸ“‚ æ­£åœ¨åŠ è½½: telemarketing_intent_cn.jsonl\n",
      "ğŸ“‚ æ­£åœ¨åŠ è½½: crosswoz.jsonl\n",
      "âœ… å·²åˆå¹¶ CrossWOZï¼Œæ€»æ ·æœ¬: 20279\n",
      "   é»‘åå•è¿‡æ»¤: 20279 â†’ 19287\n",
      "   æ ·æœ¬é˜ˆå€¼è¿‡æ»¤å: 19149 æ ·æœ¬ï¼Œ70 æ„å›¾\n",
      "   æ•°æ®å¹³è¡¡å: 9782 æ ·æœ¬\n",
      "ğŸ“¦ è®­ç»ƒé›†: 7825ï¼Œæµ‹è¯•é›†: 1957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7825/7825 [00:00<00:00, 30630.99 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1957/1957 [00:00<00:00, 30786.22 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ å¼€å§‹è®­ç»ƒ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [980/980 05:24, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.241000</td>\n",
       "      <td>1.598686</td>\n",
       "      <td>0.754727</td>\n",
       "      <td>0.723348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.100900</td>\n",
       "      <td>0.946527</td>\n",
       "      <td>0.848748</td>\n",
       "      <td>0.839414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.742100</td>\n",
       "      <td>0.749691</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.868346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.697131</td>\n",
       "      <td>0.874808</td>\n",
       "      <td>0.870780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Accuracy: 0.8748\n",
      "   F1 (weighted): 0.8708\n",
      "\n",
      "ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹...\n",
      "\n",
      "âœ… åŸºçº¿è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š\n",
      "   C:\\Users\\ASUS\\Desktop\\Yuzuriha-Rin\\assets\\models\\chinese_bert_baseline\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "hfl/chinese-bert-wwm-ext å¾®ä¿¡èŠå¤©æ„å›¾è¯†åˆ«åŸºçº¿è®­ç»ƒä»£ç ï¼ˆNotebook ä¸“ç”¨ç‰ˆï¼‰\n",
    "æ•°æ®è·¯å¾„ï¼š\n",
    "C:\\Users\\ASUS\\Desktop\\Yuzuriha-Rin\\assets\\models\\few_shot_intent_sft\\data\\\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# =================================== é…ç½® ===================================\n",
    "\n",
    "# æ•°æ®è·¯å¾„ï¼ˆä½ çš„çœŸå®è·¯å¾„ï¼‰\n",
    "DATA_DIR = Path(\n",
    "    r\"C:\\Users\\ASUS\\Desktop\\Yuzuriha-Rin\\assets\\models\\few_shot_intent_sft\\data\"\n",
    ")\n",
    "TELEMARKETING_DATA = DATA_DIR / \"telemarketing_intent_cn.jsonl\"\n",
    "CROSSWOZ_DATA = DATA_DIR / \"crosswoz.jsonl\"\n",
    "\n",
    "# è¾“å‡ºè·¯å¾„\n",
    "OUTPUT_DIR = Path(\n",
    "    r\"C:\\Users\\ASUS\\Desktop\\Yuzuriha-Rin\\assets\\models\\chinese_bert_baseline\"\n",
    ")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# è®­ç»ƒè¶…å‚æ•°\n",
    "MODEL_NAME = \"hfl/chinese-bert-wwm-ext\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "MIN_SAMPLES = 20\n",
    "MAX_SAMPLES_PER_INTENT = 300\n",
    "USE_CROSSWOZ = True\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "# =================================== é»‘åå• ===================================\n",
    "\n",
    "BLACKLIST_INTENTS = {\n",
    "    \"æŸ¥è¯¢ç±»\",\n",
    "    \"æŸ¥è¯¢(äº§å“ä¿¡æ¯)\",\n",
    "    \"æŸ¥è¯¢(ä»·æ ¼)\",\n",
    "    \"æŸ¥è¯¢(ä¼˜æƒ )\",\n",
    "    \"æŸ¥è¯¢(åº“å­˜)\",\n",
    "    \"æŸ¥è¯¢(ç‰©æµ)\",\n",
    "    \"æŸ¥è¯¢(è®¢å•)\",\n",
    "    \"æŸ¥è¯¢(è´¦æˆ·)\",\n",
    "    \"æŸ¥è¯¢(ä½™é¢)\",\n",
    "    \"å®ä½“(äº§å“)\",\n",
    "    \"å®ä½“(ä»·æ ¼)\",\n",
    "    \"å®ä½“(æ—¶é—´)\",\n",
    "    \"å®ä½“(åœ°ç‚¹)\",\n",
    "    \"å®ä½“(äººå)\",\n",
    "    \"å®ä½“(å…¬å¸)\",\n",
    "    \"å®ä½“è¯†åˆ«\",\n",
    "    \"äº§å“æ¨è\",\n",
    "    \"ä¿ƒé”€æ´»åŠ¨\",\n",
    "    \"ä¼˜æƒ ä¿¡æ¯\",\n",
    "    \"ä¸‹å•\",\n",
    "    \"æ”¯ä»˜\",\n",
    "    \"é€€æ¬¾\",\n",
    "    \"æŠ•è¯‰\",\n",
    "    \"å”®å\",\n",
    "    \"æ”¿æ²»æ•æ„Ÿ\",\n",
    "    \"æ±¡è¨€ç§½è¯­\",\n",
    "    \"è‰²æƒ…ä½ä¿—\",\n",
    "    \"æš´åŠ›è¡€è…¥\",\n",
    "    \"è¿æ³•çŠ¯ç½ª\",\n",
    "    \"å¹¿å‘Šè¥é”€\",\n",
    "    \"è¯ˆéª—ä¿¡æ¯\",\n",
    "    \"è‚¯å®š(æ²¡é—®é¢˜)\",\n",
    "    \"å¦å®š(æ²¡æœ‰)\",\n",
    "    \"è½¬äººå·¥\",\n",
    "    \"æŒ‚æ–­ç”µè¯\",\n",
    "    \"ä¿æŒé€šè¯\",\n",
    "    \"é‡å¤\",\n",
    "    \"æ¾„æ¸…\",\n",
    "    \"ç¡®è®¤ä¿¡æ¯\",\n",
    "    \"æ ¸å®èº«ä»½\",\n",
    "    \"å½•éŸ³æç¤º\",\n",
    "    \"ç³»ç»Ÿæç¤º\",\n",
    "}\n",
    "\n",
    "# =================================== æ•°æ®åŠ è½½ ===================================\n",
    "\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    if not file_path.exists():\n",
    "        print(f\"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆè·³è¿‡ï¼‰: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    print(f\"ğŸ“‚ æ­£åœ¨åŠ è½½: {file_path.name}\")\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                item = json.loads(line)\n",
    "                data.append({\"text\": item[\"text\"].strip(), \"label\": item[\"label\"]})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def prepare_dataset():\n",
    "    print(\"\\nğŸ“Š å¼€å§‹å‡†å¤‡æ•°æ®é›†...\")\n",
    "\n",
    "    df_main = load_jsonl(TELEMARKETING_DATA)\n",
    "\n",
    "    if USE_CROSSWOZ and CROSSWOZ_DATA.exists():\n",
    "        df_extra = load_jsonl(CROSSWOZ_DATA)\n",
    "        if not df_extra.empty:\n",
    "            # CrossWOZ æ„å›¾æ˜ å°„\n",
    "            mapping = {\"greet\": \"æ‹›å‘¼ç”¨è¯­\", \"thank\": \"ç¤¼è²Œç”¨è¯­\", \"bye\": \"ç»“æŸç”¨è¯­\"}\n",
    "            df_extra[\"label\"] = df_extra[\"label\"].map(mapping)\n",
    "            df_extra = df_extra.dropna(subset=[\"label\"])\n",
    "            df = pd.concat([df_main, df_extra], ignore_index=True)\n",
    "            print(f\"âœ… å·²åˆå¹¶ CrossWOZï¼Œæ€»æ ·æœ¬: {len(df)}\")\n",
    "        else:\n",
    "            df = df_main\n",
    "    else:\n",
    "        df = df_main\n",
    "\n",
    "    # é»‘åå•è¿‡æ»¤\n",
    "    before = len(df)\n",
    "    df = df[~df[\"label\"].isin(BLACKLIST_INTENTS)]\n",
    "    print(f\"   é»‘åå•è¿‡æ»¤: {before} â†’ {len(df)}\")\n",
    "\n",
    "    # æ ·æœ¬é˜ˆå€¼è¿‡æ»¤\n",
    "    intent_counts = Counter(df[\"label\"])\n",
    "    valid_intents = {k for k, v in intent_counts.items() if v >= MIN_SAMPLES}\n",
    "    df = df[df[\"label\"].isin(valid_intents)]\n",
    "    print(f\"   æ ·æœ¬é˜ˆå€¼è¿‡æ»¤å: {len(df)} æ ·æœ¬ï¼Œ{len(valid_intents)} æ„å›¾\")\n",
    "\n",
    "    # æ•°æ®å¹³è¡¡\n",
    "    balanced = []\n",
    "    for intent in df[\"label\"].unique():\n",
    "        group = df[df[\"label\"] == intent]\n",
    "        if len(group) > MAX_SAMPLES_PER_INTENT:\n",
    "            group = group.sample(n=MAX_SAMPLES_PER_INTENT, random_state=RANDOM_SEED)\n",
    "        balanced.append(group)\n",
    "    df = pd.concat(balanced, ignore_index=True)\n",
    "    print(f\"   æ•°æ®å¹³è¡¡å: {len(df)} æ ·æœ¬\")\n",
    "\n",
    "    # æ ‡ç­¾æ˜ å°„\n",
    "    intents = sorted(df[\"label\"].unique())\n",
    "    label2id = {label: idx for idx, label in enumerate(intents)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "    df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=0.2, random_state=RANDOM_SEED, stratify=df[\"label_id\"]\n",
    "    )\n",
    "\n",
    "    train_ds = Dataset.from_pandas(\n",
    "        train_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"label\"})\n",
    "    )\n",
    "    test_ds = Dataset.from_pandas(\n",
    "        test_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"label\"})\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ“¦ è®­ç»ƒé›†: {len(train_ds)}ï¼Œæµ‹è¯•é›†: {len(test_ds)}\")\n",
    "    return DatasetDict({\"train\": train_ds, \"test\": test_ds}), label2id, id2label\n",
    "\n",
    "\n",
    "# =================================== è¯„ä¼°å‡½æ•° ===================================\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "\n",
    "# =================================== å¼€å§‹è®­ç»ƒ ===================================\n",
    "\n",
    "print(\"ğŸ¯ å¼€å§‹ hfl/chinese-bert-wwm-ext åŸºçº¿è®­ç»ƒï¼ˆNotebook ç‰ˆï¼‰\\n\")\n",
    "\n",
    "# 1. å‡†å¤‡æ•°æ®\n",
    "dataset, label2id, id2label = prepare_dataset()\n",
    "num_labels = len(label2id)\n",
    "\n",
    "# 2. Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# 3. æ¨¡å‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=num_labels, label2id=label2id, id2label=id2label\n",
    ")\n",
    "\n",
    "# 4. è®­ç»ƒå‚æ•°ï¼ˆå…¼å®¹ Transformers 4.57+ï¼‰\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    eval_strategy=\"epoch\",  # æ–°ç‰ˆå‚æ•°å\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 6. å¼€å§‹è®­ç»ƒ\n",
    "print(\"\\nğŸš€ å¼€å§‹è®­ç»ƒ...\")\n",
    "trainer.train()\n",
    "\n",
    "# 7. æœ€ç»ˆè¯„ä¼°\n",
    "print(\"\\nğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ:\")\n",
    "results = trainer.evaluate()\n",
    "print(f\"   Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "print(f\"   F1 (weighted): {results['eval_f1']:.4f}\")\n",
    "\n",
    "# 8. ä¿å­˜æ¨¡å‹\n",
    "print(\"\\nğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹...\")\n",
    "trainer.save_model(str(OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "\n",
    "# ä¿å­˜æ„å›¾æ˜ å°„\n",
    "with open(OUTPUT_DIR / \"intent_mapping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\"intent2id\": label2id, \"id2intent\": id2label}, f, ensure_ascii=False, indent=2\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIR / \"intents.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"å¾®ä¿¡èŠå¤©æ„å›¾åˆ—è¡¨\\n\" + \"=\" * 50 + \"\\n\\n\")\n",
    "    for i, intent in enumerate(sorted(id2label.values())):\n",
    "        f.write(f\"{i+1:2d}. {intent}\\n\")\n",
    "\n",
    "print(f\"\\nâœ… åŸºçº¿è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š\")\n",
    "print(f\"   {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuzuriha-rin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
