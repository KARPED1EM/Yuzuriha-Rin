{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289d8697",
   "metadata": {},
   "source": [
    "# BERT WWM Ext Baseline\n",
    "This notebook fine-tunes hfl/chinese-bert-wwm-ext on the cleaned WeChat intent dataset so it stays in sync with the other training workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79741fc6",
   "metadata": {},
   "source": [
    "## Notebook Overview\n",
    "- Resolves project directories dynamically so relative paths work regardless of how the notebook is launched.\n",
    "- Loads \telemarketing_intent_cn.jsonl plus the optional crosswoz.jsonl, applies blacklist/min-sample filters, and balances label counts.\n",
    "- Fine-tunes hfl/chinese-bert-wwm-ext with Hugging Face Trainer, tracking weighted F1/accuracy during training.\n",
    "- Persists the model and label mapping artefacts under \u0007ssets/models/chinese_bert_baseline for downstream services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3eaf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Files\\Develop Projects\\AI\\Yuzuriha-Rin\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import set_seed\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_columns\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c805523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Files\\Develop Projects\\AI\\Yuzuriha-Rin\n",
      "Raw data directory: D:\\Files\\Develop Projects\\AI\\Yuzuriha-Rin\\assets\\models\\few_shot_intent_sft\\data\n",
      "Baseline artifacts: D:\\Files\\Develop Projects\\AI\\Yuzuriha-Rin\\assets\\models\\chinese_bert_baseline\n"
     ]
    }
   ],
   "source": [
    "NOTEBOOK_DIR = Path().resolve()\n",
    "\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    for candidate in [start] + list(start.parents):\n",
    "        if (candidate / \"assets\" / \"models\").exists():\n",
    "            return candidate\n",
    "    raise RuntimeError(\"Could not find project root (assets/models missing)\")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root(NOTEBOOK_DIR)\n",
    "ASSETS_DIR = PROJECT_ROOT / \"assets\"\n",
    "MODELS_DIR = ASSETS_DIR / \"models\"\n",
    "DATA_DIR = MODELS_DIR / \"few_shot_intent_sft\" / \"data\"\n",
    "BASELINE_DIR = MODELS_DIR / \"chinese_bert_baseline\"\n",
    "CHECKPOINT_DIR = BASELINE_DIR / \"checkpoints\"\n",
    "\n",
    "BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TELEMARKETING_DATA = DATA_DIR / \"telemarketing_intent_cn.jsonl\"\n",
    "CROSSWOZ_DATA = DATA_DIR / \"crosswoz.jsonl\"\n",
    "\n",
    "MODEL_NAME = \"hfl/chinese-bert-wwm-ext\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "MIN_SAMPLES = 20\n",
    "MAX_SAMPLES_PER_INTENT = 300\n",
    "USE_CROSSWOZ = True\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw data directory: {DATA_DIR}\")\n",
    "print(f\"Baseline artifacts: {BASELINE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f61004",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLACKLIST_INTENTS = {\n",
    "    \"查询类\",\n",
    "    \"查询(产品信息)\",\n",
    "    \"查询(价格)\",\n",
    "    \"查询(优惠)\",\n",
    "    \"查询(库存)\",\n",
    "    \"查询(物流)\",\n",
    "    \"查询(订单)\",\n",
    "    \"查询(账户)\",\n",
    "    \"查询(余额)\",\n",
    "    \"实体(产品)\",\n",
    "    \"实体(价格)\",\n",
    "    \"实体(时间)\",\n",
    "    \"实体(地点)\",\n",
    "    \"实体(人名)\",\n",
    "    \"实体(公司)\",\n",
    "    \"实体识别\",\n",
    "    \"产品推荐\",\n",
    "    \"促销活动\",\n",
    "    \"优惠信息\",\n",
    "    \"下单\",\n",
    "    \"支付\",\n",
    "    \"退款\",\n",
    "    \"投诉\",\n",
    "    \"售后\",\n",
    "    \"政治敏感\",\n",
    "    \"污言秽语\",\n",
    "    \"色情低俗\",\n",
    "    \"暴力血腥\",\n",
    "    \"违法犯罪\",\n",
    "    \"广告营销\",\n",
    "    \"诈骗信息\",\n",
    "    \"肯定(没问题)\",\n",
    "    \"否定(没有)\",\n",
    "    \"转人工\",\n",
    "    \"挂断电话\",\n",
    "    \"保持通话\",\n",
    "    \"重复\",\n",
    "    \"澄清\",\n",
    "    \"确认信息\",\n",
    "    \"核实身份\",\n",
    "    \"录音提示\",\n",
    "    \"系统提示\",\n",
    "}\n",
    "\n",
    "CROSSWOZ_INTENT_MAPPING = {\n",
    "    \"greet\": \"招呼用语\",\n",
    "    \"thank\": \"礼貌用语\",\n",
    "    \"bye\": \"结束用语\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae4a6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(file_path: Path) -> pd.DataFrame:\n",
    "    if not file_path.exists():\n",
    "        print(f\"Skipping missing dataset: {file_path}\")\n",
    "        return pd.DataFrame(columns=[\"text\", \"label\"])\n",
    "    rows = []\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            item = json.loads(line)\n",
    "            rows.append({\"text\": item[\"text\"].strip(), \"label\": item[\"label\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def prepare_dataset() -> tuple[DatasetDict, dict[str, int], dict[int, str]]:\n",
    "    print(\"Preparing datasets...\")\n",
    "    frames: list[pd.DataFrame] = []\n",
    "\n",
    "    main_df = load_jsonl(TELEMARKETING_DATA)\n",
    "    if main_df.empty:\n",
    "        raise FileNotFoundError(\"telemarketing_intent_cn.jsonl is required\")\n",
    "    main_df[\"source\"] = \"telemarketing\"\n",
    "    frames.append(main_df)\n",
    "\n",
    "    if USE_CROSSWOZ:\n",
    "        crosswoz_df = load_jsonl(CROSSWOZ_DATA)\n",
    "        if not crosswoz_df.empty:\n",
    "            crosswoz_df[\"label\"] = crosswoz_df[\"label\"].map(CROSSWOZ_INTENT_MAPPING)\n",
    "            crosswoz_df = crosswoz_df.dropna(subset=[\"label\"]).copy()\n",
    "            crosswoz_df[\"source\"] = \"crosswoz\"\n",
    "            frames.append(crosswoz_df)\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "    print(f\"Combined {len(df):,} rows from {df['source'].nunique()} datasets\")\n",
    "\n",
    "    before_filter = len(df)\n",
    "    df = df[~df[\"label\"].isin(BLACKLIST_INTENTS)].copy()\n",
    "    print(f\"Blacklist filter: {before_filter:,} -> {len(df):,} rows\")\n",
    "\n",
    "    intent_counts = Counter(df[\"label\"])\n",
    "    valid_labels = {\n",
    "        label for label, count in intent_counts.items() if count >= MIN_SAMPLES\n",
    "    }\n",
    "    df = df[df[\"label\"].isin(valid_labels)].copy()\n",
    "    print(f\"Minimum sample filter keeps {len(valid_labels)} intents ({len(df):,} rows)\")\n",
    "\n",
    "    balanced_parts = []\n",
    "    for label in sorted(valid_labels):\n",
    "        group = df[df[\"label\"] == label]\n",
    "        if len(group) > MAX_SAMPLES_PER_INTENT:\n",
    "            group = group.sample(n=MAX_SAMPLES_PER_INTENT, random_state=RANDOM_SEED)\n",
    "        balanced_parts.append(group)\n",
    "    df = pd.concat(balanced_parts, ignore_index=True)\n",
    "    print(\n",
    "        f\"Balanced dataset: {len(df):,} samples across {df['label'].nunique()} intents\"\n",
    "    )\n",
    "\n",
    "    intents = sorted(df[\"label\"].unique())\n",
    "    label2id = {label: idx for idx, label in enumerate(intents)}\n",
    "    id2label = {idx: label for label, idx in label2id.items()}\n",
    "    df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df[[\"text\", \"label_id\"]],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=df[\"label_id\"],\n",
    "    )\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(\n",
    "        train_df.rename(columns={\"label_id\": \"label\"}), preserve_index=False\n",
    "    )\n",
    "    test_ds = Dataset.from_pandas(\n",
    "        test_df.rename(columns={\"label_id\": \"label\"}), preserve_index=False\n",
    "    )\n",
    "\n",
    "    print(f\"Train split: {len(train_ds):,} | Test split: {len(test_ds):,}\")\n",
    "    return DatasetDict({\"train\": train_ds, \"test\": test_ds}), label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429d162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n",
      "Combined 20,279 rows from 2 datasets\n",
      "Blacklist filter: 20,279 -> 19,287 rows\n",
      "Minimum sample filter keeps 70 intents (19,149 rows)\n",
      "Balanced dataset: 9,782 samples across 70 intents\n",
      "Train split: 7,825 | Test split: 1,957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7825/7825 [00:00<00:00, 19429.37 examples/s]\n",
      "Map: 100%|██████████| 1957/1957 [00:00<00:00, 23883.90 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Leever\\AppData\\Local\\Temp\\ipykernel_5680\\1183066139.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='980' max='980' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [980/980 03:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.308000</td>\n",
       "      <td>1.629816</td>\n",
       "      <td>0.759325</td>\n",
       "      <td>0.732682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.110600</td>\n",
       "      <td>0.955364</td>\n",
       "      <td>0.850281</td>\n",
       "      <td>0.841365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.750500</td>\n",
       "      <td>0.760202</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.867925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.623500</td>\n",
       "      <td>0.699780</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.868018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating best checkpoint...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8738\n",
      "Weighted F1: 0.8680\n",
      "Saving baseline artefacts...\n",
      "Artifacts stored in: D:\\Files\\Develop Projects\\AI\\Yuzuriha-Rin\\assets\\models\\chinese_bert_baseline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6997795104980469,\n",
       " 'eval_accuracy': 0.8737864077669902,\n",
       " 'eval_f1_weighted': 0.8680175319063638,\n",
       " 'eval_runtime': 4.466,\n",
       " 'eval_samples_per_second': 438.199,\n",
       " 'eval_steps_per_second': 13.883,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, label2id, id2label = prepare_dataset()\n",
    "num_labels = len(label2id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)  # type: ignore\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(  # type: ignore\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1_weighted\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(  # type: ignore\n",
    "    output_dir=str(CHECKPOINT_DIR),\n",
    "    eval_strategy=\"epoch\",  # type: ignore\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    "    seed=RANDOM_SEED,\n",
    ")\n",
    "\n",
    "trainer = Trainer(  # type: ignore\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,  # type: ignore\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"Evaluating best checkpoint...\")\n",
    "final_metrics = trainer.evaluate()\n",
    "print(f\"Accuracy: {final_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"Weighted F1: {final_metrics['eval_f1_weighted']:.4f}\")\n",
    "\n",
    "print(\"Saving baseline artefacts...\")\n",
    "trainer.save_model(str(BASELINE_DIR))\n",
    "tokenizer.save_pretrained(str(BASELINE_DIR))\n",
    "\n",
    "mapping_path = BASELINE_DIR / \"intent_mapping.json\"\n",
    "intents_txt = BASELINE_DIR / \"intents.txt\"\n",
    "with mapping_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\"intent2id\": label2id, \"id2intent\": {str(k): v for k, v in id2label.items()}},\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "with intents_txt.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"WeChat intent label list\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    for idx, intent in enumerate(sorted(label2id.keys()), start=1):\n",
    "        f.write(f\"{idx:02d}. {intent}\\n\")\n",
    "\n",
    "print(f\"Artifacts stored in: {BASELINE_DIR}\")\n",
    "final_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yuzuriha-rin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
